{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec52c4f",
   "metadata": {},
   "source": [
    "# Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb79d1",
   "metadata": {},
   "source": [
    "- Try 10–20 minutes on a problem without hints.\n",
    "\n",
    "- Use tutorial/ChatGPT if stuck. Ask ChatGPT for:\n",
    "\n",
    "    - Why a solution works.\n",
    "\n",
    "    - Alternative approaches.\n",
    "\n",
    "    - General patterns for similar problems.\n",
    "\n",
    "- Re-solve the problem later without looking at hints to cement the concept.\n",
    "\n",
    "- Focus on patterns rather than isolated problems; this is what really makes you “master” LeetCode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d5ac42",
   "metadata": {},
   "source": [
    "## Feb 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc3f7c4",
   "metadata": {},
   "source": [
    "### Two Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def twoSum(self, nums: List[int], target: int) -> List[int]:\n",
    "        i = 0\n",
    "        d = {}\n",
    "\n",
    "        for i in range(len(nums)):\n",
    "            bal = target - nums[i]\n",
    "            if bal in d:\n",
    "                return [d[bal], i]\n",
    "            d[nums[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649891ed",
   "metadata": {},
   "source": [
    "### Longest Common Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def longestCommonPrefix(self, strs: List[str]) -> str:\n",
    "        i = 0\n",
    "        com_prefix = \"\"\n",
    "\n",
    "        while True:\n",
    "            if i < len(strs[0]):\n",
    "                curr = strs[0][i]\n",
    "            else:\n",
    "                return com_prefix\n",
    "\n",
    "            for s in strs[1:]:\n",
    "                if not (len(s) > i and curr == s[i]):\n",
    "                    return com_prefix\n",
    "\n",
    "            com_prefix += curr\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ced95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def longestCommonPrefix(self, strs: List[str]) -> str:\n",
    "        if not strs:\n",
    "            return \"\"\n",
    "        \n",
    "        # zip(*strs) pairs characters by index: (char1, char2, char3...)\n",
    "        for i, chars in enumerate(zip(*strs)):\n",
    "            # If the set of characters at this index > 1, they aren't all the same\n",
    "            if len(set(chars)) > 1:\n",
    "                return strs[0][:i]\n",
    "        \n",
    "        # If the loop finishes, the shortest string is the prefix\n",
    "        return min(strs, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c7dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "strs = [\"flower\", \"flow\", \"flight\"]\n",
    "for i, chars in enumerate(zip(*strs)):\n",
    "    print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c7c807",
   "metadata": {},
   "source": [
    "## Feb 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c834133",
   "metadata": {},
   "source": [
    "### 26. Remove Duplicates from Sorted Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eade1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDuplicates(nums):\n",
    "    length = len(nums)\n",
    "    if length <= 1:\n",
    "        return length\n",
    "        \n",
    "    i, j = 0, 1\n",
    "\n",
    "    while j < length:\n",
    "        if nums[i] == nums[j]:\n",
    "            while j < length and nums[i] == nums[j]:\n",
    "                j += 1\n",
    "            if j < length:\n",
    "                nums[i+1], nums[j] = nums[j], nums[i+1]\n",
    "            else:\n",
    "                break\n",
    "        i += 1\n",
    "        j += 1\n",
    "    print(nums)\n",
    "    return i + 1\n",
    "\n",
    "nums = [0, 0, 1, 1, 1, 2, 2, 3, 3, 4]\n",
    "nums = [1, 1, 2]\n",
    "nums = [1, 1]\n",
    "nums = [0, 1, 1, 1, 1]\n",
    "nums = [1, 1, 2, 3]\n",
    "removeDuplicates(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d9e94",
   "metadata": {},
   "source": [
    "## Feb 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800568a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDuplicates(nums):\n",
    "    l, r = 1, 1\n",
    "\n",
    "    while r < len(nums):\n",
    "        if nums[r-1] == nums[r]:\n",
    "            r += 1\n",
    "        else:\n",
    "            nums[l] = nums[r]\n",
    "            l += 1\n",
    "            r += 1\n",
    "    return l\n",
    "\n",
    "nums = [1, 1, 2, 3]\n",
    "removeDuplicates(nums)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd8379",
   "metadata": {},
   "source": [
    "## Feb 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62017271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def removeElement(self, nums: List[int], val: int) -> int:\n",
    "        k = 0\n",
    "        for i in range(len(nums)):\n",
    "            if nums[i] != val:\n",
    "                nums[k] = nums[i]\n",
    "                k += 1\n",
    "        return k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f8bad",
   "metadata": {},
   "source": [
    "## Feb 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba0064",
   "metadata": {},
   "source": [
    "### 35. Search Insert Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ed62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def searchInsert(self, nums: List[int], target: int) -> int:\n",
    "        for i in range(len(nums)):\n",
    "            if nums[i] == target \\\n",
    "            or nums[i] > target:\n",
    "                return i\n",
    "\n",
    "        return len(nums) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c63f1b",
   "metadata": {},
   "source": [
    "### 66. Plus One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cfb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def plusOne(self, digits: List[int]) -> List[int]:         \n",
    "        digits_str = [str(i) for i in digits]\n",
    "        ans = str(int(\"\".join(digits_str)) + 1)\n",
    "        ans = [int(i) for i in ans]\n",
    "        return ans\n",
    "\n",
    "# the above is not recommended for large numbers\n",
    "# the below is recommended from gpt\n",
    "class Solution:\n",
    "    def plusOne(self, digits: List[int]) -> List[int]:\n",
    "        n = len(digits)\n",
    "        \n",
    "        # Start from the last digit\n",
    "        for i in range(n - 1, -1, -1):\n",
    "            if digits[i] < 9:\n",
    "                digits[i] += 1\n",
    "                return digits\n",
    "            digits[i] = 0  # carry over\n",
    "        \n",
    "        # If all digits were 9\n",
    "        return [1] + [0] * n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3541243",
   "metadata": {},
   "source": [
    "### 88. Merge Sorted Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e97b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -> None:\n",
    "        \"\"\"\n",
    "        Do not return anything, modify nums1 in-place instead.\n",
    "        \"\"\"\n",
    "        merge = []\n",
    "        l, r = 0, 0\n",
    "        while l < m and r < n:\n",
    "            if nums1[l] <= nums2[r]:\n",
    "                merge.append(nums1[l])\n",
    "                l += 1\n",
    "            elif nums1[l] > nums2[r]:\n",
    "                merge.append(nums2[r])\n",
    "                r += 1\n",
    "        while l < m:\n",
    "            merge.append(nums1[l])\n",
    "            l += 1\n",
    "        \n",
    "        while r < n:\n",
    "            merge.append(nums2[r])\n",
    "            r += 1\n",
    "\n",
    "        for i in range(len(merge)):\n",
    "            nums1[i] = merge[i]\n",
    "\n",
    "        return merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b708a56",
   "metadata": {},
   "source": [
    "While the above solves th problem, it uses extra space O(m+n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7fc78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt solution\n",
    "\n",
    "class Solution:\n",
    "    def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -> None:\n",
    "        i = m - 1      # last valid element in nums1\n",
    "        j = n - 1      # last element in nums2\n",
    "        k = m + n - 1  # write pointer\n",
    "\n",
    "        while i >= 0 and j >= 0:\n",
    "            if nums1[i] > nums2[j]:\n",
    "                nums1[k] = nums1[i]\n",
    "                i -= 1\n",
    "            else:\n",
    "                nums1[k] = nums2[j]\n",
    "                j -= 1\n",
    "            k -= 1\n",
    "\n",
    "        # Copy remaining nums2 elements (if any)\n",
    "        while j >= 0:\n",
    "            nums1[k] = nums2[j]\n",
    "            j -= 1\n",
    "            k -= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840dba85",
   "metadata": {},
   "source": [
    "## Feb 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342bbcf1",
   "metadata": {},
   "source": [
    "### 108. Convert Sorted Array to Binary Search Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcda88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition for a binary tree node.\n",
    "# class TreeNode:\n",
    "#     def __init__(self, val=0, left=None, right=None):\n",
    "#         self.val = val\n",
    "#         self.left = left\n",
    "#         self.right = right\n",
    "class Solution:\n",
    "    def BST(self, nums, left, right):\n",
    "\n",
    "        if left > right:\n",
    "            return None\n",
    "\n",
    "        mid = (left + right)//2\n",
    "\n",
    "        mid_node = TreeNode(nums[mid])\n",
    "        mid_node.left = self.BST(nums, left, mid - 1)\n",
    "        mid_node.right = self.BST(nums, mid + 1, right)\n",
    "        return mid_node\n",
    "\n",
    "    def sortedArrayToBST(self, nums: List[int]) -> Optional[TreeNode]:\n",
    "\n",
    "        return self.BST(nums, 0, len(nums) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3100e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tune GPT-OSS-20b on AIMO3 Tool-Integrated Reasoning Dataset using TRL and PEFT.\n",
    "\n",
    "This script uses:\n",
    "- LoRA for parameter-efficient fine-tuning\n",
    "- TRL's SFTTrainer for supervised fine-tuning\n",
    "- Gradient checkpointing and mixed precision for efficiency\n",
    "\n",
    "Note: gpt-oss-20b is already quantized with MXFP4, so we don't apply\n",
    "additional quantization (no BitsAndBytes).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Mxfp4Config,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration.\"\"\"\n",
    "    model_name_or_path: str = \"openai/gpt-oss-20b\"\n",
    "    torch_dtype: str = \"bfloat16\"\n",
    "    use_flash_attention_2: bool = False  # Disabled - not supported\n",
    "    trust_remote_code: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LoRAConfig:\n",
    "    \"\"\"LoRA configuration.\"\"\"\n",
    "    r: int = 32  # LoRA rank\n",
    "    lora_alpha: int = 64  # LoRA alpha\n",
    "    lora_dropout: float = 0.0\n",
    "    bias: str = \"none\"\n",
    "    task_type: str = \"CAUSAL_LM\"\n",
    "    target_modules: str = \"all-linear\"\n",
    "    target_parameters: list = field(default_factory=lambda: [\n",
    "        \"mlp.experts\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Data Processing\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def format_example_prompt_completion(example: dict) -> dict:\n",
    "    \"\"\"Format as prompt-completion pair for SFTTrainer.\n",
    "    \n",
    "    According to TRL docs, prompt-completion format allows loss to be computed\n",
    "    only on completion tokens, which is ideal for instruction following.\n",
    "    \"\"\"\n",
    "    problem = example.get(\"problem\", \"\")\n",
    "    solution = example.get(\"solution\", \"\")\n",
    "    \n",
    "    # Format prompt with instruction\n",
    "    prompt = f\"\"\"Please reason step by step to solve this math problem.\n",
    "Return the final answer in \\\\boxed{{}}.\n",
    "\n",
    "{problem}\"\"\"\n",
    "\n",
    "    return {\"prompt\": [{\"role\": \"user\", \"content\": prompt}],\n",
    " \"completion\": [{\"role\": \"assistant\", \"content\": solution}]}\n",
    "\n",
    "\n",
    "def load_csv_dataset(\n",
    "    csv_path: str,\n",
    "    tokenizer,\n",
    "    max_samples: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"Load dataset from CSV file with prompt and completion columns.\n",
    "    \n",
    "    The CSV should have 'prompt' and 'completion' columns:\n",
    "    - 'prompt': Raw prompt text (problem + instructions) without special tokens\n",
    "    - 'completion': Assistant response already formatted with Harmony format tags\n",
    "    \n",
    "    The prompt will be formatted using tokenizer.apply_chat_template in the formatting function.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to CSV file\n",
    "        tokenizer: Tokenizer to use for chat template formatting\n",
    "        max_samples: Maximum number of samples to load\n",
    "    \n",
    "    Returns:\n",
    "        Dataset with 'prompt' and 'completion' fields\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset from CSV: {csv_path}\")\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if 'prompt' not in df.columns or 'completion' not in df.columns:\n",
    "        raise ValueError(f\"CSV must have 'prompt' and 'completion' columns. Found columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    print(f\"Loaded {len(df)} samples from CSV\")\n",
    "    \n",
    "    # Limit samples if requested\n",
    "    if max_samples is not None:\n",
    "        df = df.head(max_samples)\n",
    "        print(f\"Limited to {len(df)} samples\")\n",
    "    \n",
    "    # Convert to HuggingFace Dataset\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    print(f\"Dataset features: {dataset.features}\")\n",
    "    print(f\"Sample prompt length (first row): {len(dataset[0]['prompt'])} chars\")\n",
    "    print(f\"Sample completion length (first row): {len(dataset[0]['completion'])} chars\")\n",
    "    print(f\"Sample prompt preview:\\n{dataset[0]['prompt'][:200]}...\")\n",
    "    print(f\"Sample completion preview:\\n{dataset[0]['completion'][:200]}...\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Model Setup\n",
    "# ============================================================================\n",
    "\n",
    "def load_model_and_tokenizer(\n",
    "    model_config: ModelConfig,\n",
    "    lora_config: LoRAConfig,\n",
    "):\n",
    "    \"\"\"Load model and prepare for LoRA training.\n",
    "    \n",
    "    Note: gpt-oss-20b is already quantized with MXFP4. We use Mxfp4Config\n",
    "    to explicitly control quantization behavior. With LoRA/PEFT, only adapter\n",
    "    weights are trained while base weights remain quantized.\n",
    "    \"\"\"\n",
    "    print(f\"Loading model: {model_config.model_name_or_path}\")\n",
    "    print(\"Note: Model is pre-quantized with MXFP4\")\n",
    "    \n",
    "    # Check if Triton is available\n",
    "    try:\n",
    "        import triton\n",
    "        if hasattr(triton, '__version__'):\n",
    "            print(f\"Triton version: {triton.__version__}\")\n",
    "        else:\n",
    "            print(\"Triton is installed but version cannot be determined\")\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  WARNING: Triton is not installed. The model may be dequantized, causing OOM.\")\n",
    "        print(\"   Install with: pip install triton>=3.4.0\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_config.model_name_or_path,\n",
    "        trust_remote_code=model_config.trust_remote_code,\n",
    "    )\n",
    "    \n",
    "    # Set padding token if not set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Configure MXFP4 quantization\n",
    "    # Set dequantize=False to keep weights quantized (or True if gradients need dequantized weights)\n",
    "    quantization_config = Mxfp4Config(dequantize=True)\n",
    "    \n",
    "    # Determine torch dtype\n",
    "    torch_dtype = getattr(torch, model_config.torch_dtype)\n",
    "    \n",
    "    # Model kwargs similar to the provided pattern\n",
    "    model_kwargs = dict(\n",
    "        attn_implementation=\"eager\",  # Use eager attention (flash_attention_2 not supported)\n",
    "        dtype=torch_dtype,\n",
    "        quantization_config=quantization_config,\n",
    "        use_cache=False,  # Disable cache for training\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=model_config.trust_remote_code,\n",
    "    )\n",
    "    \n",
    "    # Load model with explicit MXFP4 config\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_config.model_name_or_path,\n",
    "        **model_kwargs\n",
    "    )\n",
    "\n",
    "    # What we want to adapt\n",
    "    PATTERNS = [\n",
    "        r\"\\.self_attn\\.(q_proj|k_proj|v_proj|o_proj)$\",\n",
    "        r\"\\.mlp\\.experts\\.gate_up_projs\\.\\d+$\",        # every expert's up proj\n",
    "        r\"\\.mlp\\.experts\\.down_projs\\.\\d+$\",           # every expert's down proj\n",
    "    ]\n",
    "    PATTERNS = [re.compile(p) for p in PATTERNS]\n",
    "    \n",
    "    def is_target(name, module):\n",
    "        if not isinstance(module, (nn.Linear, bnb.nn.Linear4bit)):\n",
    "            return False\n",
    "        return any(p.search(name) for p in PATTERNS)\n",
    "    \n",
    "    target_modules = [name for name, mod in model.named_modules() if is_target(name, mod)]\n",
    "    print(f\"{len(target_modules)} modules will get LoRA\")\n",
    "    \n",
    "    \n",
    "    # Configure LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        r=lora_config.r,\n",
    "        lora_alpha=lora_config.lora_alpha,\n",
    "        lora_dropout=lora_config.lora_dropout,\n",
    "        bias=lora_config.bias,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    \n",
    "    # Get PEFT model\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer, peft_config\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Training\n",
    "# ============================================================================\n",
    "\n",
    "def create_training_arguments(args) -> SFTConfig:\n",
    "    \"\"\"Create training arguments.\"\"\"\n",
    "    return SFTConfig(\n",
    "        output_dir=args.output_dir,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        lr_scheduler_type=args.lr_scheduler_type,\n",
    "        max_length=args.max_seq_length,\n",
    "        \n",
    "        # Optimization\n",
    "        optim=\"adamw_torch\",  # Standard optimizer (no paged_adamw since no bnb)\n",
    "        bf16=True,\n",
    "        fp16=False,\n",
    "        gradient_checkpointing=True,\n",
    "        \n",
    "        # Logging\n",
    "        logging_steps=args.logging_steps,\n",
    "        logging_first_step=True,\n",
    "        report_to=args.report_to,\n",
    "        \n",
    "        # Saving\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=args.save_steps,\n",
    "        save_total_limit=2,\n",
    "        \n",
    "        # Evaluation\n",
    "        eval_strategy=\"no\",\n",
    "        \n",
    "        # Other\n",
    "        seed=args.seed,\n",
    "        dataloader_num_workers=4,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_cache_path(csv_path: str, max_length: int, output_dir: str) -> str:\n",
    "    \"\"\"Generate a cache path for the tokenized dataset.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to the CSV file\n",
    "        max_length: Maximum sequence length used for tokenization\n",
    "        output_dir: Output directory for training (used as base for cache)\n",
    "    \n",
    "    Returns:\n",
    "        Path to the cache directory\n",
    "    \"\"\"\n",
    "    import hashlib\n",
    "    \n",
    "    # Create a hash from CSV path and max_length to ensure unique cache per dataset/config\n",
    "    cache_key = f\"{csv_path}_{max_length}\"\n",
    "    cache_hash = hashlib.md5(cache_key.encode()).hexdigest()[:8]\n",
    "    \n",
    "    # Use output_dir as base, create a cache subdirectory\n",
    "    csv_basename = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "    cache_dir = os.path.join(output_dir, f\"tokenized_cache_{csv_basename}_{cache_hash}\")\n",
    "    \n",
    "    return cache_dir\n",
    "\n",
    "\n",
    "def try_load_cached_dataset(csv_path: str, max_length: int, output_dir: str):\n",
    "    \"\"\"Try to load cached tokenized dataset if it exists and is valid.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to the CSV file\n",
    "        max_length: Maximum sequence length used for tokenization\n",
    "        output_dir: Output directory for training (used as base for cache)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (dataset, loaded_from_cache) where dataset is the loaded dataset\n",
    "        or None if cache doesn't exist/invalid, and loaded_from_cache is a boolean\n",
    "    \"\"\"\n",
    "    cache_path = get_cache_path(csv_path, max_length, output_dir)\n",
    "    \n",
    "    if not os.path.exists(cache_path):\n",
    "        return None, False\n",
    "    \n",
    "    print(f\"Found cached tokenized dataset at: {cache_path}\")\n",
    "    print(\"Loading cached tokenized dataset...\")\n",
    "    try:\n",
    "        dataset = load_from_disk(cache_path)\n",
    "        print(f\"âœ… Successfully loaded cached tokenized dataset ({len(dataset)} samples)\")\n",
    "        # Verify the cached dataset has the expected columns\n",
    "        if not all(col in dataset.column_names for col in ['input_ids', 'attention_mask', 'labels']):\n",
    "            print(\"âš ï¸  Warning: Cached dataset missing expected columns. Will re-tokenize...\")\n",
    "            return None, False\n",
    "        print(f\"Cached dataset columns: {dataset.column_names}\")\n",
    "        return dataset, True\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error loading cached dataset: {e}\")\n",
    "        print(\"Will re-tokenize dataset...\")\n",
    "        return None, False\n",
    "\n",
    "\n",
    "def create_custom_tokenize_function(tokenizer, max_length: int):\n",
    "    \"\"\"Create a tokenization function that formats prompts and masks prompt tokens.\n",
    "    \n",
    "    The function:\n",
    "    1. Applies chat template to prompts: tokenizer.apply_chat_template([{'role': 'user', 'content': prompt}], ...)\n",
    "    2. Concatenates formatted prompt with completion (which already has Harmony format tags)\n",
    "    3. Tokenizes the full text\n",
    "    4. Creates labels, masking prompt tokens (setting them to -100)\n",
    "    \n",
    "    The final text format is: {formatted_prompt}<|start|>assistant...\n",
    "    We want to mask the formatted prompt part, keeping only completion tokens for loss computation.\n",
    "    \"\"\"\n",
    "    def tokenize_function(examples):\n",
    "        prompts = examples['prompt']\n",
    "        completions = examples['completion']\n",
    "        \n",
    "        # Format prompts using chat template and create full texts\n",
    "        formatted_texts = []\n",
    "        formatted_prompts = []  # Store formatted prompts separately for masking\n",
    "        for prompt, completion in zip(prompts, completions):\n",
    "            # Apply chat template to format the prompt\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                [{'role': 'user', 'content': prompt}],\n",
    "                add_generation_prompt=False,\n",
    "                tokenize=False\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "            # Concatenate formatted prompt with completion (completion already has Harmony tags)\n",
    "            full_text = formatted_prompt + completion\n",
    "            formatted_texts.append(full_text)\n",
    "        \n",
    "        # Tokenize the texts\n",
    "        tokenized = tokenizer(\n",
    "            formatted_texts,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,  # Return lists, not tensors\n",
    "        )\n",
    "        \n",
    "        # Create labels, masking prompt tokens\n",
    "        labels = []\n",
    "        for i, (formatted_prompt, full_text) in enumerate(zip(formatted_prompts, formatted_texts)):\n",
    "            input_ids = tokenized['input_ids'][i]\n",
    "            \n",
    "            # Find where the prompt ends by tokenizing the formatted prompt separately\n",
    "            # The prompt ends where the completion begins\n",
    "            try:\n",
    "                # Tokenize just the formatted prompt to find its length\n",
    "                prompt_tokenized = tokenizer(\n",
    "                    formatted_prompt,\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    padding=False,\n",
    "                    add_special_tokens=False,\n",
    "                    return_tensors=None,\n",
    "                )\n",
    "                prompt_end_idx = len(prompt_tokenized['input_ids'])\n",
    "            except Exception:\n",
    "                # Fallback: use string search to find where completion starts\n",
    "                # Completion starts with <|start|>assistant\n",
    "                completion_start_marker = '<|start|>assistant'\n",
    "                completion_start_pos = full_text.find(completion_start_marker)\n",
    "                if completion_start_pos >= 0:\n",
    "                    # Tokenize prefix up to completion start\n",
    "                    prefix_text = full_text[:completion_start_pos]\n",
    "                    try:\n",
    "                        prefix_tokenized = tokenizer.encode(\n",
    "                            prefix_text,\n",
    "                            add_special_tokens=False,\n",
    "                            truncation=True,\n",
    "                            max_length=max_length\n",
    "                        )\n",
    "                        prompt_end_idx = len(prefix_tokenized)\n",
    "                    except Exception:\n",
    "                        prompt_end_idx = len(input_ids) // 5  # Fallback: mask first 20%\n",
    "                else:\n",
    "                    prompt_end_idx = len(input_ids) // 5  # Fallback: mask first 20%\n",
    "            \n",
    "            # Ensure prompt_end_idx doesn't exceed input_ids length\n",
    "            prompt_end_idx = min(prompt_end_idx, len(input_ids))\n",
    "            \n",
    "            # Create labels: -100 for prompt tokens (masked), input_ids for completion\n",
    "            label = [-100] * prompt_end_idx + list(input_ids[prompt_end_idx:])\n",
    "            # Ensure same length\n",
    "            if len(label) < len(input_ids):\n",
    "                label = label + [-100] * (len(input_ids) - len(label))\n",
    "            elif len(label) > len(input_ids):\n",
    "                label = label[:len(input_ids)]\n",
    "            labels.append(label)\n",
    "        \n",
    "        tokenized['labels'] = labels\n",
    "        return tokenized\n",
    "    \n",
    "    return tokenize_function\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    training_args: SFTConfig,\n",
    "    peft_config: LoraConfig,\n",
    "    merge_and_unload: bool = True,\n",
    "    use_csv_format: bool = False,\n",
    "    csv_path: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Run training.\n",
    "    \n",
    "    Args:\n",
    "        model: The PEFT model to train\n",
    "        tokenizer: Tokenizer for the model\n",
    "        dataset: Training dataset\n",
    "        training_args: SFTConfig with training arguments\n",
    "        peft_config: LoRA configuration\n",
    "        merge_and_unload: If True, merge LoRA adapters into base model after training\n",
    "        use_csv_format: If True, dataset has 'prompt' and 'completion' fields (prompt will be formatted with chat template)\n",
    "        csv_path: Path to CSV file (used for cache path generation)\n",
    "    \"\"\"\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    if use_csv_format:\n",
    "        # For CSV data with prompt/completion columns, we apply chat template to prompts\n",
    "        print(\"Using CSV format with prompt/completion columns\")\n",
    "        print(\"Chat template will be applied to prompts, completions already have Harmony format tags\")\n",
    "        \n",
    "        # Check if dataset is already tokenized (has input_ids, attention_mask, labels columns)\n",
    "        # This happens when dataset was loaded from cache in main()\n",
    "        is_already_tokenized = all(col in dataset.column_names for col in ['input_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        if is_already_tokenized:\n",
    "            print(\"Dataset is already tokenized (loaded from cache in main()), skipping tokenization step\")\n",
    "            cache_path = get_cache_path(csv_path, training_args.max_length, training_args.output_dir) if csv_path else None\n",
    "            loaded_from_cache = True\n",
    "        else:\n",
    "            # Dataset is not tokenized, need to tokenize it\n",
    "            loaded_from_cache = False\n",
    "            cache_path = get_cache_path(csv_path, training_args.max_length, training_args.output_dir) if csv_path else None\n",
    "        \n",
    "        if not loaded_from_cache:\n",
    "            # Create tokenization function that masks prompt tokens\n",
    "            tokenize_fn = create_custom_tokenize_function(tokenizer, training_args.max_length)\n",
    "            \n",
    "            # Tokenize the dataset\n",
    "            print(\"Tokenizing dataset and masking prompt tokens...\")\n",
    "            print(\"Applying chat template to prompts and concatenating with completions...\")\n",
    "            # Remove columns we don't need for training (keep 'prompt' and 'completion' for tokenization)\n",
    "            columns_to_remove = [col for col in dataset.column_names if col not in ['prompt', 'completion']]\n",
    "            if columns_to_remove:\n",
    "                dataset = dataset.remove_columns(columns_to_remove)\n",
    "            # Tokenize (applies chat template, creates input_ids, attention_mask, labels) and remove 'prompt' and 'completion'\n",
    "            dataset = dataset.map(\n",
    "                tokenize_fn,\n",
    "                batched=True,\n",
    "                remove_columns=['prompt', 'completion'],  # Remove after tokenization creates input_ids, attention_mask, labels\n",
    "                desc=\"Formatting prompts, tokenizing and masking\"\n",
    "            )\n",
    "            \n",
    "            # Save tokenized dataset to cache\n",
    "            if csv_path and cache_path:\n",
    "                print(f\"Saving tokenized dataset to cache: {cache_path}\")\n",
    "                os.makedirs(cache_path, exist_ok=True)\n",
    "                try:\n",
    "                    dataset.save_to_disk(cache_path)\n",
    "                    print(f\"âœ… Successfully saved tokenized dataset to cache\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸  Warning: Failed to save cached dataset: {e}\")\n",
    "                    print(\"   Training will continue, but cache won't be available for next run\")\n",
    "        \n",
    "        # Use DataCollatorForLanguageModeling with mlm=False for causal LM\n",
    "        # This will handle padding and batching, while respecting our labels\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False,  # Causal language modeling, not masked LM\n",
    "        )\n",
    "        \n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            processing_class=tokenizer,\n",
    "            train_dataset=dataset,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            #dataset_text_field=None,  # We've already tokenized, so don't use text field\n",
    "        )\n",
    "    else:\n",
    "        # Original format with prompt/completion pairs\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            processing_class=tokenizer,\n",
    "            train_dataset=dataset,\n",
    "            args=training_args,\n",
    "            #peft_config=peft_config,\n",
    "        )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train(resume_from_checkpoint=False)\n",
    "    \n",
    "    # Save PEFT adapter (this saves only the LoRA weights)\n",
    "    print(f\"Saving PEFT adapter to {training_args.output_dir}\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    \n",
    "    # Optionally merge and unload\n",
    "    if merge_and_unload:\n",
    "        print(\"Merging LoRA adapters into base model...\")\n",
    "        merged_model = trainer.model.merge_and_unload()\n",
    "        \n",
    "        # Save merged model to a separate directory\n",
    "        merged_output_dir = f\"{training_args.output_dir}_merged\"\n",
    "        print(f\"Saving merged model to {merged_output_dir}\")\n",
    "        merged_model.save_pretrained(merged_output_dir)\n",
    "        tokenizer.save_pretrained(merged_output_dir)\n",
    "        print(f\"âœ… Merged model saved to {merged_output_dir}\")\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main\n",
    "# ============================================================================\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Fine-tune GPT-OSS-20b on NuminaMath-CoT\"\n",
    "    )\n",
    "    \n",
    "    # Model arguments\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        type=str,\n",
    "        default=\"openai/gpt-oss-20b\",\n",
    "        help=\"Path to the base model (default: openai/gpt-oss-20b)\",\n",
    "    )\n",
    "    \n",
    "    # Output arguments\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"./gpt-oss-finetuned\",\n",
    "        help=\"Output directory for the fine-tuned model\",\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--per_device_train_batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=8)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-4)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
    "    parser.add_argument(\"--warmup_ratio\", type=float, default=0.03)\n",
    "    parser.add_argument(\"--lr_scheduler_type\", type=str, default=\"cosine\")\n",
    "    parser.add_argument(\"--max_seq_length\", type=int, default=8192)\n",
    "    parser.add_argument(\"--csv_path\", type=str, default=\"data.csv\")\n",
    "    \n",
    "    # LoRA arguments\n",
    "    parser.add_argument(\"--lora_r\", type=int, default=16)\n",
    "    parser.add_argument(\"--lora_alpha\", type=int, default=32)\n",
    "    parser.add_argument(\"--lora_dropout\", type=float, default=0.02)\n",
    "    \n",
    "    # Logging arguments\n",
    "    parser.add_argument(\"--logging_steps\", type=int, default=1)\n",
    "    parser.add_argument(\"--save_steps\", type=int, default=20)\n",
    "    parser.add_argument(\"--report_to\", type=str, default=\"none\")\n",
    "    \n",
    "    # Data arguments\n",
    "    parser.add_argument(\"--max_samples\", type=int, default=None)\n",
    "    parser.add_argument(\n",
    "        \"--quick\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Use 0.1%% of the dataset for quick testing (overrides --max_samples)\",\n",
    "    )\n",
    "    \n",
    "    # Model saving arguments\n",
    "    parser.add_argument(\n",
    "        \"--merge_and_unload\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Merge LoRA adapters into base model after training and save merged model\",\n",
    "    )\n",
    "    \n",
    "    # Other\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function.\"\"\"\n",
    "    args = parse_args()\n",
    "    \n",
    "    # Set seed\n",
    "    set_seed(args.seed)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"gpt-oss-20b Fine-tuning on NuminaMath-CoT\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Model: {args.model_name_or_path}\")\n",
    "    print(f\"Output: {args.output_dir}\")\n",
    "    print(f\"LoRA r={args.lora_r}, alpha={args.lora_alpha}\")\n",
    "    print(f\"Batch size: {args.per_device_train_batch_size} x {args.gradient_accumulation_steps}\")\n",
    "    print(f\"Learning rate: {args.learning_rate}\")\n",
    "    print(f\"Epochs: {args.num_train_epochs}\")\n",
    "    if args.quick:\n",
    "        print(\"âš ï¸  Quick mode enabled: Using 0.1% of dataset\")\n",
    "    print(\"Note: Model is pre-quantized with MXFP4 (no additional quantization)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create configurations\n",
    "    model_config = ModelConfig(model_name_or_path=args.model_name_or_path)\n",
    "    lora_config_obj = LoRAConfig(\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "    )\n",
    "    \n",
    "    # Load model and tokenizer (without additional quantization)\n",
    "    model, tokenizer, peft_config = load_model_and_tokenizer(\n",
    "        model_config, lora_config_obj\n",
    "    )\n",
    "    \n",
    "    # Chat template will be applied in the custom tokenization function\n",
    "    if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template is not None:\n",
    "        print(f\"Using tokenizer chat_template for prompt formatting\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Warning: Tokenizer does not have chat_template. Prompt formatting may not work correctly.\")\n",
    "    \n",
    "    # Create training arguments (needed for cache path generation)\n",
    "    training_args = create_training_arguments(args)\n",
    "    \n",
    "    # Try to load cached tokenized dataset first (skips CSV loading and formatting if cache exists)\n",
    "    use_csv_format = args.csv_path is not None\n",
    "    dataset = None\n",
    "    \n",
    "    if use_csv_format:\n",
    "        # Check for cached tokenized dataset before loading CSV\n",
    "        cached_dataset, loaded_from_cache = try_load_cached_dataset(\n",
    "            args.csv_path, args.max_seq_length, args.output_dir\n",
    "        )\n",
    "        if loaded_from_cache:\n",
    "            dataset = cached_dataset\n",
    "            print(\"âœ… Skipping CSV loading and formatting - using cached tokenized dataset\")\n",
    "        else:\n",
    "            # Cache doesn't exist or is invalid, load from CSV\n",
    "            dataset = load_csv_dataset(args.csv_path, tokenizer, max_samples=args.max_samples)\n",
    "    else:\n",
    "        raise ValueError(\"Dataset must be a CSV file\")\n",
    "\n",
    "    # Train\n",
    "    trainer = train(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        dataset, \n",
    "        training_args, \n",
    "        peft_config,\n",
    "        use_csv_format=use_csv_format,\n",
    "        csv_path=args.csv_path if use_csv_format else None,\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"PEFT adapter saved to: {args.output_dir}\")\n",
    "    if args.merge_and_unload:\n",
    "        print(f\"Merged model saved to: {args.output_dir}_merged\")\n",
    "    else:\n",
    "        print(\"\\nTo merge adapters later, use:\")\n",
    "        print(f\"  from transformers import AutoModelForCausalLM\")\n",
    "        print(f\"  from peft import PeftModel\")\n",
    "        print(f\"  base_model = AutoModelForCausalLM.from_pretrained('{args.model_name_or_path}', ...)\")\n",
    "        print(f\"  model = PeftModel.from_pretrained(base_model, '{args.output_dir}')\")\n",
    "        print(f\"  model = model.merge_and_unload()\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11560c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
